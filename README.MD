# 🧠 Sign Language Detection using Action Recognition

> A deep learning project that uses **MediaPipe Holistic**, **LSTM Neural Networks**, and **Action Recognition** to detect and classify American Sign Language (ASL) gestures like `Hello`, `Thank You`, and `I Love You` in real-time from webcam input.

---

## 🚀 Project Highlights

- 🔍 **Real-time Pose, Hand & Face Tracking** using MediaPipe Holistic.
- 🤖 **Action Classification** via LSTM-based neural networks trained on sequential keypoints.
- 📈 Achieved high accuracy on custom dataset with 3-class gesture recognition.
- 💾 Organized training pipeline with NumPy-based keypoint extraction.
- 🎨 Live prediction visualization using OpenCV.

---

## 🛠 Tech Stack

| Tool/Library        | Purpose                                 |
|---------------------|-----------------------------------------|
| `MediaPipe`         | Real-time holistic human pose tracking  |
| `OpenCV`            | Image capture and display               |
| `TensorFlow/Keras`  | LSTM model training and prediction      |
| `NumPy`             | Feature vector generation               |
| `Matplotlib`        | Data visualization                      |
| `Scikit-learn`      | Model evaluation and preprocessing      |

---


